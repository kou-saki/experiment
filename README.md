# Token Limit Investigation

This branch contains a series of experiments related to token limit behavior in GPT models (primarily GPT-4-turbo).

## ğŸ“Œ Purpose

The purpose of this investigation is to:

- Understand how GPT models behave near their token output limits
- Compare different character types (e.g., Japanese vs. English, frequent vs. rare)
- Observe compression patterns, truncation behavior, and output stability

## ğŸ“ Files Included

| File Name | Description |
|-----------|-------------|
| `Token_Limit_Investigation_Single_Character_Repeat.md` | A markdown-based summary of output behavior across character types |
| `ãƒˆãƒ¼ã‚¯ãƒ³é™ç•Œèª¿æŸ»å®Ÿé¨“ï¼šå‡ºåŠ›æ¯”è¼ƒã¾ã¨ã‚.md` | Japanese summary of output comparison results |
| `ãƒˆãƒ¼ã‚¯ãƒ³é™ç•Œå€¤æ¢æŸ»_æ—¥æœ¬èªæœ€é »_ã„.txt` | Output from repeating the most frequent Japanese character ("ã„") |
| `ãƒˆãƒ¼ã‚¯ãƒ³é™ç•Œå€¤æ¢æŸ»_æ—¥æœ¬èªæœ€å°é »_ã¬.txt` | Output from repeating a rare Japanese character ("ã¬") |
| `ãƒˆãƒ¼ã‚¯ãƒ³é™ç•Œå€¤æ¢æŸ»_è‹±èªæœ€é »_e.txt` | Output from repeating the most frequent English letter ("e") |
| `ãƒˆãƒ¼ã‚¯ãƒ³é™ç•Œå€¤æ¢æŸ»_è‹±èªæœ€å°é »_z.txt` | Output from repeating a rare English letter ("z") |

## ğŸ”¬ Notes

- This experiment is part of a broader investigation into the internal mechanics of GPT tokenization and output management.
- The outputs were generated using ChatGPT (GPT-4-turbo) with default settings.
- No external preprocessing or formatting was applied to raw outputs.

## ğŸ§  Motivation

The experiment aims to surface reproducible insights for:

- Prompt engineers
- AI researchers
- Anyone trying to understand GPT's token limits in practice

---

If you want to reproduce this experiment, we recommend:
- Using the ChatGPT interface (or API) with a prompt structure that encourages max-length generation.
- Comparing character types across different languages and frequency distributions.

---

## :page_facing_up:	LICENSE

- This project is licensed under the MIT License.
